"""
GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding
https://openreview.net/pdf?id=rJ4km2R5t7

The General Language Understanding Evaluation (GLUE) benchmark is a collection of
resources for training, evaluating, and analyzing natural language understanding
systems. GLUE consists of:
- A benchmark of nine sentence- or sentence-pair language understanding tasks built
on established existing datasets and selected to cover a diverse range of dataset
sizes, text genres, and degrees of difficulty, and
- A diagnostic dataset designed to evaluate and analyze model performance with
respect to a wide range of linguistic phenomena found in natural language.

Homepage: https://gluebenchmark.com/
"""
import abc
import numpy as np
from lm_eval.base import rf, Task, PerplexityTask, MultipleChoiceTask
from lm_eval.metrics import mean, matthews_corrcoef, f1_score, yesno
from lm_eval.metrics import mean, weighted_perplexity, weighted_mean, bits_per_byte
from lm_eval.utils import general_detokenize
from lm_eval import metrics
from rouge import Rouge
import re
import inspect
from megatron.tokenizer.tokenizer import HFGPTNeoXTokenizerFast


rouger = Rouge()
ROOT_PATH = "/cognitive_comp/common_data/gpt-neox-eval/"
ACA_PATH = "/cognitive_comp/yangqi/data/eval_data/"

def rouge_l(items):
    hyps, refs = map(
        list, zip(*[[" ".join(list(d[0])), " ".join(list(d[1]))] for d in items])
    )
    scores = rouger.get_scores(hyps, refs, avg=True)
    return scores


class GenerateTask(Task):
    VERSION = 0
    DATASET_PATH = None
    DATASET_NAME = None

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        return doc["input"]

    def should_decontaminate(self):
        return False

    def doc_to_decontamination_query(self, doc):
        return doc["question"]

    def doc_to_target(self, doc):
        return "\n\n" + doc["output"]

    def construct_requests(self, doc, ctx):
        continuation = rf.greedy_until(ctx, ["<|endoftext|>"])
        return continuation

    def process_results(self, doc, results):
        ref_pred = (doc["output"], results)
        ref_pred_rouge = (
            (doc["output"], results[0] + "ã€‚")
            if results[0] == ""
            else (doc["output"], results[0])
        )
        return {
            "bleu": ref_pred,
            "rouge_l": ref_pred_rouge,
            "chrf": ref_pred,
            "ter": ref_pred,
        }

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        return {
            "bleu": metrics.bleu,
            "rouge_l": rouge_l,
            "chrf": metrics.chrf,
            "ter": metrics.ter,
        }

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {
            "bleu": True,
            "rouge_l": True,
            "chrf": True,
            "ter": False,
        }


class ClassificationTask(Task):
    VERSION = 0
    DATASET_PATH = None
    DATASET_NAME = None

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):

        return doc["input"]

    def should_decontaminate(self):
        return False

    def doc_to_decontamination_query(self, doc):
        return doc["premise"]

    def doc_to_target(self, doc):
        return "\n\n" + [doc["output"]]

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        choice_scores = []
        for c in doc["choice"]:
            score, _ = rf.loglikelihood(ctx, "\n\n" + c)
            choice_scores.append(score)
        return choice_scores

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        gold = doc["label"]
        pred = np.argmax(results)
        return {"acc": pred == gold}

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        return {"acc": mean}

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {"acc": True}


class FSMultipleChoiceTask(MultipleChoiceTask):
    VERSION = 0
    DATASET_PATH = None
    DATASET_NAME = None

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    # def validation_docs(self):
    #     return map(self._process_doc, self.dataset["validation"])

    def test_docs(self):
        return map(self._process_doc, self.dataset["test"])

    def _process_doc(self, doc):
        out_doc = {
            "query": doc["input"],
            "choices": doc["choice"],
            "gold": doc["label"],
        }
        return out_doc

    def doc_to_text(self, doc):
        return doc["query"]

    def should_decontaminate(self):
        return True

    def doc_to_decontamination_query(self, doc):
        return doc["query"]


class FSPerplexityTask(PerplexityTask):
    def higher_is_better(self):
        return {
            "word_perplexity": False,
            "byte_perplexity": False,
            "bits_per_byte": False,
        }

    def process_results(self, doc, results):
        (loglikelihood,) = results
        words = self.count_words(doc)
        bytes_ = self.count_bytes(doc)
        return {
            "word_perplexity": (loglikelihood, words),
            "byte_perplexity": (loglikelihood, bytes_),
            "bits_per_byte": (loglikelihood, bytes_),
        }

    def aggregation(self):
        return {
            "word_perplexity": weighted_perplexity,
            "byte_perplexity": weighted_perplexity,
            "bits_per_byte": bits_per_byte,
        }

    @classmethod
    def count_token(cls, doc):
        """Downstream tasks with custom word boundaries should override this!"""
        return len(tokenizer.tokenize(doc))


class PilePerplexityTask(FSPerplexityTask):
    VERSION = 1
    DATASET_PATH = None
    DATASET_NAME = None

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def validation_docs(self):
        for doc in self.dataset["validation"]:
            yield doc["text"]

    def test_docs(self):
        for doc in self.dataset["test"]:
            yield doc["text"]


class USDPerplexityTask(FSPerplexityTask):
    VERSION = 0
    DATASET_PATH = None
    DATASET_NAME = None

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def validation_docs(self):
        for doc in self.dataset["validation"]:
            yield doc["content"]

    def test_docs(self):
        for doc in self.dataset["test"]:
            yield doc["content"]

    def process_results(self, doc, results):
        (loglikelihood,) = results
        words = self.count_words(doc)
        bytes_ = self.count_bytes(doc)
        return {
            "char_perplexity": (loglikelihood, words),
            "byte_perplexity": (loglikelihood, bytes_),
            "bits_per_byte": (loglikelihood, bytes_),
        }

    def aggregation(self):
        return {
            "char_perplexity": weighted_perplexity,
            "byte_perplexity": weighted_perplexity,
            "bits_per_byte": bits_per_byte,
        }

    def higher_is_better(self):
        return {
            "char_perplexity": False,
            "byte_perplexity": False,
            "bits_per_byte": False,
        }


class PileArxiv(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_arxiv"


class PileBooks3(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_books3"


class PileBookCorpus2(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_bookcorpus2"


class PileDmMathematics(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_dm-mathematics"


class PileEnron(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_enron"


class PileEuroparl(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_europarl"


class PileFreeLaw(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_freelaw"


class PileGithub(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_github"


class PileGutenberg(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_gutenberg"


class PileHackernews(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_hackernews"


class PileNIHExporter(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_nih-exporter"


class PileOpenSubtitles(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_opensubtitles"


class PileOpenWebText2(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_openwebtext2"


class PilePhilPapers(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_philpapers"


class PilePileCc(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_pile-cc"


class PilePubmedAbstracts(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_pubmed-abstracts"


class PilePubmedCentral(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_pubmed-central"


class PileStackExchange(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_stackexchange"


class PileUspto(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_upsto"


class PileUbuntuIrc(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_ubuntu-irc"


class PileWikipedia(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_wikipedia"


class PileYoutubeSubtitles(PilePerplexityTask):
    DATASET_PATH = ROOT_PATH + "pile_youtubesubtitles"


class webqa(GenerateTask):
    DATASET_PATH = ROOT_PATH + "webqa"


class cmqa(GenerateTask):
    DATASET_PATH = ROOT_PATH + "cmqa"


class lcsts(GenerateTask):
    DATASET_PATH = ROOT_PATH + "lcsts"


class translate_en2zh(GenerateTask):
    DATASET_PATH = ROOT_PATH + "translate_en2zh"


class translate_zh2en(GenerateTask):
    DATASET_PATH = ROOT_PATH + "translate_zh2en"


class c3(FSMultipleChoiceTask):
    DATASET_PATH = ROOT_PATH + "c3"


class ocnli(FSMultipleChoiceTask):
    DATASET_PATH = ROOT_PATH + "ocnli"


class bustm(FSMultipleChoiceTask):
    DATASET_PATH = ROOT_PATH + "bustm"


class eprstmt(FSMultipleChoiceTask):
    DATASET_PATH = ROOT_PATH + "eprstmt"


class afqmc(FSMultipleChoiceTask):
    DATASET_PATH = ROOT_PATH + "afqmc"


class chid(FSMultipleChoiceTask):
    DATASET_PATH = ROOT_PATH + "chid"


class wsc(FSMultipleChoiceTask):
    DATASET_PATH = ROOT_PATH + "wsc"


class cmnli(FSMultipleChoiceTask):
    DATASET_PATH = ROOT_PATH + "cmnli"


class zh_baike(USDPerplexityTask):
    DATASET_PATH = ROOT_PATH + "baike"


class zh_cnki(USDPerplexityTask):
    DATASET_PATH = ROOT_PATH + "cnki_summary"


class zh_copywriting(USDPerplexityTask):
    DATASET_PATH = ROOT_PATH + "copywriting"


class zh_wiki(USDPerplexityTask):
    DATASET_PATH = ROOT_PATH + "wiki_zh"


class zh_zhihu(USDPerplexityTask):
    DATASET_PATH = ROOT_PATH + "zhihu"


class zh_wudao(USDPerplexityTask):
    DATASET_PATH = ROOT_PATH + "wudao"


class zh_medical(USDPerplexityTask):
    DATASET_PATH = ROOT_PATH + "medical_data"

# arxiv

class AcaMultipleChoiceTask(MultipleChoiceTask): 
    """alignment with key, withou a special few-shot dev set"""
    VERSION = 0
    DATASET_PATH = None
    DATASET_NAME = None

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    # def validation_docs(self):
    #     return map(self._process_doc, self.dataset["validation"])

    def test_docs(self):
        return map(self._process_doc, self.dataset["test"])

    def doc_to_text(self, doc):
        return doc["query"]

    def should_decontaminate(self):
        return True

    def doc_to_decontamination_query(self, doc):
        return doc["query"]
    
class MMLU(AcaMultipleChoiceTask):
    DATASET_PATH = ACA_PATH + "mmlu"
    
class CevalMC(AcaMultipleChoiceTask):
    DATASET_PATH = ACA_PATH + "ceval_mc"
    
class PaperQA(AcaMultipleChoiceTask):
    DATASET_PATH = ACA_PATH + "paper_qa"
    
TASK_REGISTRY = {
    "webqa": webqa,
    "cmqa": cmqa,
    "translate_en2zh": translate_en2zh,
    "translate_zh2en": translate_zh2en,
    "lcsts": lcsts,
    "c3": c3,
    "ocnli": ocnli,
    "cmnli": cmnli,
    "afqmc": afqmc,
    "bustm": bustm,
    "wsc": wsc,
    "eprstmt": eprstmt,
    "chid": chid,
    "zh_baike": zh_baike,
    "zh_cnki": zh_cnki,
    "zh_copywriting": zh_copywriting,
    "zh_wiki": zh_wiki,
    "zh_zhihu": zh_zhihu,
    "zh_wudao": zh_wudao,
    "zh_medical": zh_medical,
    "pile_arxiv": PileArxiv,
    "pile_books3": PileBooks3,
    "pile_bookcorpus2": PileBookCorpus2,
    "pile_dm-mathematics": PileDmMathematics,
    "pile_enron": PileEnron,
    "pile_europarl": PileEuroparl,
    "pile_freelaw": PileFreeLaw,
    "pile_github": PileGithub,
    "pile_gutenberg": PileGutenberg,
    "pile_hackernews": PileHackernews,
    "pile_nih-exporter": PileNIHExporter,
    "pile_opensubtitles": PileOpenSubtitles,
    "pile_openwebtext2": PileOpenWebText2,
    "pile_philpapers": PilePhilPapers,
    "pile_pile-cc": PilePileCc,
    "pile_pubmed-abstracts": PilePubmedAbstracts,
    "pile_pubmed-central": PilePubmedCentral,
    "pile_stackexchange": PileStackExchange,
    "pile_uspto": PileUspto,
    "pile_ubuntu-irc": PileUbuntuIrc,
    "pile_wikipedia": PileWikipedia,
    "pile_youtubesubtitles": PileYoutubeSubtitles,
    "aca_paper_qa":PaperQA,
    "aca_ceval_mc":CevalMC,
    "aca_mmlu": MMLU
}

all_tasks_zh = [k for k in TASK_REGISTRY.keys()]


def get_task(task_name):
    try:
        return TASK_REGISTRY[task_name]
    except KeyError:
        print("Available tasks:")
        pprint(TASK_REGISTRY)
        raise KeyError(f"Missing task {task_name}")


def get_task_dict_zh(task_name_list):
    task_name_dict = {
        task_name: get_task(task_name)()
        for task_name in task_name_list
        if isinstance(task_name, str)
    }
    return task_name_dict
